\documentclass[thesis.tex]{subfiles}

\begin{document}

\chapter{Background}
\label{chap:background-information}

In this chapter we provide an introduction to the relevant background material pertaining
to this thesis. We begin with a discussion of \emph{decision support systems}, followed by
an overview of \emph{artificial reasoning}. Next we discuss \emph{Dempster-Shafer Theory}, and
then \emph{Subjective Logic}. We then conclude with
a brief overview of the \emph{Haskell} programming language, as it is the language used for the program
examples throughout this thesis.



\section{Decision Support Systems}

Decision support systems are information systems that are designed to aide users with various
decision-making tasks \cite{sprague1980framework}. Examples of such tasks are those pertaining to management,
planning, or operations. Typically decision support systems work with the kinds of unstructured
or underspecified problems faced by managers and decision-makers in many areas; involve the
synthesis of models, analytics, and data; are targetted at non-technical people; and are designed
to be flexible and adaptable in the face of new data or changes to the working environment
\cite{sprague1980framework}. Some example decision support systems are the
Unified Data Management and Decision Support System (UDMDSS)
for health research surveys \cite{kent2010application},
and the Gate Assignment Display System (GADS) developed for United Airlines
by Texas Instruments [cite].

In his 2002 book, \emph{Decision support systems: concepts and resources for managers}
\cite{power2002decision}, Daniel J Power breaks down Decision support systems into the following
taxonomy:

\begin{itemize}
  \item Communication-driven systems: systems that allow for more than
    one person to work on a shared task.
  \item Document-driven systems: systems that allow for the storage,
    retrieval and manipulation of unstructured data documents.
  \item Data-driven systems: systems that facilitate the manipulation
    of internal company data.
  \item Model-driven systems: systems that allow for access and
    modification of various models: whether they are financial,
    simulation, statistical, or other.
  \item Knowledge-driven systems: systems that contain problem solving
    expertise for the task at hand, typically encoded as facts and
    rules.
\end{itemize}

%
% Discuss UDMDSS and its components. We develop a full-spectrum DSS that requires reasoning
% at every step. SL forms the core of the reasoning engine. How does this library fit in with UDSS?
%


%
% Seguay this into reasoning systems...
%






%It is in the latter, \emph{knowledge-driven systems}, that this thesis is mostly concerned with: systems
%that are required to reason with (possibly incomplete) domain knowledge in order to assist the human
%decision-maker with his or her task.





\section{Artificial Reasoning}






%
% Deduction: We all do deduction. Truths + Inference Rule = more facts.
% Induction: The scientist. Observe something, generate hypothesis, test hypothesis.
% Abduction: The doctor. Observe something, list of hypotheses, need to induce the
% correct one and then apply deduction to see if it is valid? More info on this...
%



%
% Certain vs Uncertain reasoning.
% Certain: closed world. Propositional logic, modal logic, etc.
% Uncertain: open world. Most of the weight of the thesis goes into this section...
%





Unlike deduction, induction and abduction are not certain inference techniques: it is
not possible to reason inductively or abductively with absolute certainty.

\subsection{Uncertain Reasoning}

Dealing with the various kinds of uncertainty, such as fuzzy or incomplete information,
has been a cornerstone problem for artificial reasoning research. Various systems and
calculi have been proposed and explored over the years, including but not limited to

\begin{itemize}
  \item Bayesian Probability
  \item Fuzzy logic and fuzzy set theory
  \item Dempster-Shafer Theory
  \item Subjective Logic
\end{itemize}




\subsubsection{Bayesian Probability}

%
% This is not Bayes' rule. This is bayesian updating. Introduce this better.
% Contrast this with frequentist statistics?
%

%
% mention the complexity of inverting the conditionals for bayesian update.
%

Bayesian probability is a probabilistic extension to propositional logic that
supports reasoning with propositions whose truth values are uncertain. Bayesian
probability is an evidence-based probability theory: probabilities of events are set
to prior values and these values are updated as new evidence is encountered
by applying \emph{Bayes' Rule}. The probability of $A$ when accounting for $B$ is
computed as

$$
P(A | B) = \frac{P(B | A)}{P(B)} \times P(A)
$$

where $\frac{P(B | A)}{P(B)}$ represents the support that $B$ gives to $A$, and
$P(A)$ is the prior probability of $A$.





\subsubsection{Fuzzy Set Theory}

%
% Discuss overlapping instances. Example: ``Medium'' vs ``Tall''. How does
% an item being in two sets at the same time correspond to some uncertainty...
%

Fuzzy set theory is an extension to traditional set theory that allows for
elements to be partially contained within sets. In traditional set theory,
the statement $x \in A$ is binary: $x$ is either an element of the set $A$ or
it is not an element of $A$. For a given fuzzy set, fuzzy set theory assigns
a measure to each element between 0 and 1, where 0 means the element is not
in the set, and 1 means the element is entirely in the set, and any value in
between represents a kind of partial membership.

As an example, consider the set of tall people. The concept of tallness is
somewhat ambiguous, and so we represent it as a fuzzy set by the following
membership function:

$$
Tall(X) = \begin{cases}
  0                 & \mbox{if } x < 42 \\
  \frac{x - 42}{28} & \mbox{if } 42 \leq x \leq 70 \\
  1                 & \mbox{if } x > 70
\end{cases}
$$

If x, the persons height in inches, is less than 42, then they are definitely
not tall. If the person is taller than 6 feet, then they are definitely tall.
Any height in between is assigned a value between 0 and 1 by a simple linear
function.





\subsubsection{Dempster-Shafer Theory}

Dempster-Shafer Theory is a mathematical and philosophical theory of evidence in
which the central model of calculation is the \emph{basic belief assignment}. Given
a \emph{frame of discernment} - a set of mutually disjoint (atomic) elements - that
represents the entire set of discernable states in the universe of discourse, a
basic belif assignment is a function from the powerset of the frame of discernment
into the range $\lbrack 0, 1\rbrack$. The associated \emph{belief mass} for each
subset of the frame is a measure of the amount of belief given by the holder of
the basic belief assignment to the subset of states of the universe. Basic belief
assignments can be combined together using \emph{Dempster's rule of combination}
to produce new basic belief assignments. Subjective Logic shares many commonalities
with Dempster-Shafer Theory, and thus we discuss Dempster-Shafer Theory in more detail
in the next section.



\subsubsection{Subjective Logic}

Subjective Logic was introduced by Adun Josang in 1997 [cite] as a new
extension to probabilistic logic that fixes some of the issues with
Dempster-Shafer Theory. Though it is relatively young and is under
constant refinement, Subjective Logic has been shown to be effective
across a range of areas that require uncertain reasoning.  The primary
object of consideration in Subjective Logic is the Subjective Opinion,
which is a 3-tuple: $\omega_X = \langle b_X, u_X, a_X \rangle$, where
$b_X$ is an assignment of belief mass to proper non-empty subsets of
the frame of discernment $X$, $u_X$ is a scalar representing the
uncertainty of the opinion, and $a_X$ is the a-priori belief mass
assigned to elements of the frame $X$. Subjective Logic supports many
operators for combining opinions together, and has support for both
deductive and abductive reasoning with opinions.

In this section we introduced the topic of artifical reasoning and discussed
four major calculi for reasoning with uncertainty. In the next section we
discuss Dempster-Shafer Theory in more detail, and then we give a more
thourough introduction to Subjective Logic.






%
% DS-Theory and SL allow for ``evidentiary reasoning''. Perhaps add a section and
% label it as so.
%

\section{Dempster-Shafer Theory}

Dempster-Shafer Theory is a mathematical and philosophical theory of evidence that allows for
the combination of evidence from different sources to arrive at a level of
belief \cite{shafer1976mathematical}.

Let $X$ be a \emph{frame of discernment},
a set of mutually atomic entities. A \emph{basic belief assignment} over X is a function $m$ from the
powerset of X to $[0, 1]$. A basic belief assignment assigns \emph{belief mass} to subsets of the frame
of discernment. Basic belief assignments are additive: $\sum_{x \in 2^X} m(x) = 1$,
and no mass is assigned to the empty set: $m(\emptyset) = 0$.

Given a subset A of the frame of discernment X and a basic belief assignment $m$, the precise probability
of A can be bounded above and below by its \emph{belief} and its \emph{plausibility}:

\begin{itemize}
  \item $bel(A) = \sum_{B \subseteq A} m(B)$
  \item $pl(A) = 1 - bel(\overline{A})$
\end{itemize}

Given two basic belief assignments $m_1$ and $m_2$ representing two pieces of evidence related to the
same frame of discernment $X$, we can combine them together using \emph{Dempster's Rule of Combination}. The
\emph{joint mass} of $m_1$ and $m_2$, denoted as $m_{1,2}$, is calculated via the following equation:

\begin{equation*}
  \begin{split}
    m_{1,2}\left(\emptyset\right) & = 0 \\
    m_{1,2}\left(A\right)         & = \left( m_1 \otimes m_2\right) = \frac{1}{1 - K} \sum_{B \cap C = A \neq \emptyset} m_1(B) m_2(C)
  \end{split}
\end{equation*}

where $K$, the amount of conflict between $m_1$ and $m_2$, is $\sum_{B \cap C = \emptyset} m_1(B) m_2(C)$. While
fairly straight forward to calculate, it has been shown by Zadeh \cite{zadeh1979validity, zadeh1986simple}
that Dempster's Rule generates counter-intiutive results when there is a high degree of
conflict between the two belief masses, and Josang and Pope claimed that Dempster's Rule actually
represents a method of preference combination while serving as an approximation for other forms of belief
combination \cite{josang2012dempster}. Subjective Logic, which we will introduce next, contains several
operators for combining beliefs together \cite{josang2012interpretation, josang2010cumulative, josang2009fission, josang2009cumulative},
and thus offers more tools to the user than Dempster-Shafer theory.





\section{Subjective Logic}

Subjective Logic is a newly emergent probabilistic logic calculus that supports reasoning
under uncertainty \cite{josang2001logic}. In this section we will briefly introduce the
core objects of Subjective Logic: subjective opinions, as well as some of the operators that
allow for the construction of complex expressions.

\subsection{Subjective Opinions}

The primary building blocks of Subjective Logic are
objects called \emph{subjective opinions}. Given a frame of discernment $\Theta$, a subjective
opinion over $\Theta$ is a 3-tuple consisting of the following elements:

\begin{itemize}
  \item A \emph{belief vector}, $b_\Theta$, of assigned belief mass that spans the \emph{reduced power set}
    of $\Theta$. The reduced power set is defined as $R \left(\Theta\right) = 2^\Theta \setminus \lbrace \Theta, \emptyset \rbrace$.
  \item A scalar, $u_\Theta$, that represents the unassigned belief mass.
    $u_\Theta + \sum_{x \in R\left(\Theta\right)} b_\Theta\left(x\right) = 1$.
  \item A vector of a-priori belief, $a_\Theta$, that spans the frame $\Theta$.
\end{itemize}

such that the following conditions hold:

\begin{enumerate}
  \item $\forall x \in R\left(\Theta\right), b_\Theta\left(x\right) \in \lbrack 0, 1\rbrack$
  \item $\forall x \in \Theta, a_\Theta\left(x\right) \in \lbrack 0, 1\rbrack$
  \item $u_\Theta \in \lbrack 0, 1\rbrack$
  \item $u_\Theta + \sum_{x \in R\left(\Theta\right)} b_\Theta\left(x\right) = 1$
  \item $\sum_{x \in \Theta} a_\Theta\left(x\right) = 1$
\end{enumerate}

Opinions are written as $\omega^A_\Theta = \langle b^A_\Theta, u^A_\Theta, a^A_\Theta \rangle$, where
$A$ is the (optional) agent who holds that particular belief.

Elements of $R\left(\Theta\right)$ such that $b_\Theta\left(x\right) > 0$ are called \emph{focal elements}.
Subjective opinions where the focal elements are all singleton sets - that is, every focal element is
simply an element of $\Theta$ - are refered to as \emph{multinomial opinions}. Multinomial opinions
defined over frames of cardinality 2 are refered to as \emph{binomial opinions}. The most general of
opinions, subjective opinions, are also refered to as \emph{hyper opinions}. Lastly, opinions can either
be \emph{dogmatic}, when $u_\Theta$ is zero, or \emph{uncertain} otherwise. The six classes of subjective
opinions are summarized in table \ref{tbl:sl-opinions}.

\begin{table}
  \begin{center}
    \begin{tabular}{ r|c|c|c| }
      \multicolumn{1}{r}{}
      &  \multicolumn{1}{c}{$|\Theta| = 2$}
      &  \multicolumn{1}{c}{$|\Theta| > 2$}
      &  \multicolumn{1}{c}{$|R(\Theta)| = 2^{|\Theta|} - 2$} \\
      \cline{2-4}
      $u > 0$ & Uncertain Binomial & Uncertain Multinomial & Uncertain Hyper \\
      \cline{2-4}
      $u = 0$ & Dogmatic Binomial & Dogmatic Multinomial & Dogmatic Hyper \\
      \cline{2-4}
    \end{tabular}
  \end{center}

  \caption{Subjective Logic Opinions}
  \label{tbl:sl-opinions}
\end{table}

Binomial opinions have a special notation that is used to emphasize the binary nature of the frame of
discernment. Given a frame $\Theta = \lbrace x, \lnot x \rbrace$, the binomial opinion of $x$ is written
as $\omega_x = \langle b_x, d_x, u_x, a_x \rangle$, where

\begin{itemize}
  \item $b_x$ is the belief of event $x$ being true.
  \item $d_x$ is the belief of event $x$ being false.
  \item $u_x$ is the uncertainty of whether $x$ is true or false.
  \item $a_x$ is the belief of $x$ being true prior to the collection of evidence.
\end{itemize}

Opinions in Subjective Logic can be mapped to and from probability distributions from Probability
Theory. Binomial opinions correspond to \emph{beta distributions}, multinomial opinions correspond
to \emph{dirichlet distributions}, and hyper opinions correspond to \emph{hyper-dirichlet distributions}.


\subsection{Subjective Logic Operators}

Subjective Logic includes a wealth of operators for working with all kinds of opinions. It includes
the traditional binary logic operators such as \emph{and}, \emph{or}, and \emph{not}, which are
designed to incorporate uncertainty, as well as the set-theoretic operators \emph{union} and
\emph{set-difference}. In the case of absolute belief ($b_x = 1$) or disbelief ($d_x =1$), these
binomial operators behave the same as they would in traditional logic.

Subjective logic also includes operators for working with multinomial opinions, such as
cumulative and averaging \emph{fusion} and \emph{unfusion}. These operators allow for combining
multinomial opinions from different sources. Subjective Logic also includes operators for performing
transitive trust analysis, where an agent A has an opinion of agent B, and agent B has an opinion of
the event X. Agent A, through its opinion of agent B, can derive an opinion of event X by using one
of several \emph{discounting} operators. Subjective Logic also includes an operator for
\emph{belief constraining}, which can be used when multiple agents need to reach a consensus opinion.
This operator is in fact equivalent in meaning to Dempster's rule of combination \cite{josang2012dempster}.

Lastly, Subjective Logic also includes operators for performing uncertain reasoning. It includes
\emph{deduction} and \emph{abduction} operators for subjective opinions, thereby allowing Subjective
Logic to be used for intelligence analysis \cite{pope2005analysis},
bayesian network analysis \cite{josang2008conditional}, and other actions that
require reasoning when uncertainty is present.




\section{Languages and Tools for Artificial Reasoning}


% TODO: Group these into languages and tools.




In this section we discuss some tools that are available for researchers developing
artificial reasoning systems.

\subsection{Weka}

Waikato Environment for Knowledge Analysis (WEKA) is a popular workbench for machine learning
\cite{witten2005data}. It contains many popular algorithms and visualization techniques for performing data mining,
data analysis, and predictive modelling. It is developed in the JAVA programming language, and is
distributed as \emph{Free Software} under the GNU General Public License.

%
% Written in Java. Talk about weaknesses vs haskell.
%

\subsection{R}

R is a programming language for statistical computing [cite]. It is popular among statisticians, and
is a power and free alternative to other statistical tools such as SAS and SPSS [cite]. R can be extended through
user-defined packages, many of which are available through repositories such as CRAN and Bioconductor.

%
% Talk about being built mainly for statisticians. General-purpose but not as nice as Haskell.
%

\subsection{DSI Toolbox}

Dempster-Shafer with Intervals (DSI) is a verified MATLAB toolbox
for computing with Dempster-Shafer Theory \cite{auer2010verified}. The authors
claim that DSI introduces intervals to a previously developed IPP toolbox\cite{limbourg2007}
that fixes rounding errors. The authors claim that their use of interval calculus and
directed rounding allows DSI to avoid the rounding errors of IPP.

%
% Fixes rounding errors. What other errors could possibly exist? This is a workbench though,
% so compare it vs ours.
%

\subsection{Prolog}

Prolog is a general purpose declarative programming language that represents all computations
as either facts, or Horn-clauses, and programs are executed by chaining together logical inferences.
Horn clauses are logical statements of the form

$$
head :- X1, X2, ... XN
$$

meaning the statement $head$ is true only when statements $X1$ through $XN$ are also true.
As an example of how one can represent computations in Prolog, the following program computes
the factorial of a number:

\begin{code}
factorial(0, X) :- X = 1.
factorial(N, X) :- NN = N - 1, factorial(NN, X1), X = X1 * N.
\end{code}

Though it was the language of choice for Japan's ambitious fifth generation computing project \cite{shapiro1983fifth},
Prolog still sees much use in the Natural Language Processing community
\cite{covington1994natural, pelletier2006representation},
as it has excellent support for implementing \emph{definite-clause grammars} \cite{pereira1980definite}.

%
% Fuck Prolog. Discuss writing general purpose programs. etc.
%



%
% TODO: Add a section on agent-based modelling.
%



%
% TODO: Add a table of each toolkit / language discussed. Rows:
% 1. Tool name
% 2. Fundamental item for reasoning
% 3. Input Data
% 4. Drawbacks.
%




\subsection{Summary}

There currently exist many tools for developing artificial reasoning systems, yet
due to it being quite young in comparrison to other calculi, there do not yet exist any
comprehensive tools for developing applications with Subjective Logic. In the next section
we present an overview of the Haskell programming language, and in chapter \ref{chap:sl-in-haskell}
we present SLHS: Subjective Logic in Haskell.






\section{Functional Programming in Haskell}

\emph{Haskell} is a strongly typed, non-strict, pure functional
programming language \cite{hudak1992report} which was initially
developed to be a common language for researchers interested in
non-strict, pure functional programming languages
\cite{hudak2007history}.
By \emph{non-strict}, we mean that Haskell
evaluates expressions in a \emph{call-by-need} manner: expressions are
only evaluated if and when they are required [cite]. Haskell is a
\emph{functional programming language}, where the meaning of
\emph{functional} is the style of programs as described by John Backus
in his Turing award lecture: \emph{Can Programming Be Liberated from
  the von Neumann Style?}\cite{backus1978can}.  Lastly, Haskell is
\emph{pure} in the sense that all functions are functions in the
mathematical sense: they depend only on their inputs to produce their
outputs. Haskell does not support the use of global variables when
writing programs.

In this section we will briefly describe the syntax of Haskell in
order to give the reader enough familiarity to understand the code
listings of chapter \ref{chap:sl-in-haskell}. This section is by no
means exhaustive in its treatment of Haskell. For readers who wish to
learn Haskell in more depth, we suggest the book \emph{Real World
  Haskell}.

Functions in Haskell are written as equations, with parameters separated
by white space. For example, the function to compute factorials can be
written as

\begin{spec}
factorial 0 = 1
factorial n = n * factorial (n - 1)
\end{spec}

All expressions in Haskell have \emph{types}. For example, the type of the literal
5 is \emph{Int}. Syntactically this is expressed as \emph{5 :: Int}. The function
\emph{factorial} above has the type \emph{factorial :: Int $\rightarrow$ Int}.

Lists in Haskell are enclosed in square braces, and their elements must be of all
the same type. As an example, the following is a valid list:

\begin{spec}
names :: [String]
names = [''John'', ''Paul'', ''George'', ''Ringo'']
\end{spec}

whereas the following is invalid:

\begin{spec}
things = [5, ''seven'', 2/3]
\end{spec}

Types in Haskell can be organized into \emph{Type Classes}, where each type in a
type class must have certain required operations defined over it. For example, consider
the following class:

\begin{spec}
class Monoid n where
    id  :: n
    (<>) :: n -> n -> n
\end{spec}

which states that a type $n$ satisfies the properties of being a \emph{Monoid} if there exists
a element $id$ of type $n$, and there exists an operator for combining elements of type $n$.
Unfortunately the additional requirement of associativity cannot be expressed in Haskell.
Instances of the Monoid class can then be defined for individual types:

\begin{spec}
instance Monoid Int where
    id = 0
    x <> y = x + y
\end{spec}

One type class in particular gets special attention in Haskell. Types that are instances of
class \emph{Monad} are very popular in functional programming, and Haskell in particular. Monads
are mathematical objects from \emph{category theory} that are prevalent throughout Haskell. Most
importantly, Haskell uses monads to model imperative state [cite], which allows Haskell to read
input from the user, and send output to the computer screen, while remaining a pure functional
language. Types that are instances of \emph{Monad} require two operations to be present:

\begin{spec}
class Monad m where
    return :: a -> m a
    (>>=)  :: m a -> (a -> m b) -> m b
\end{spec}

The first function, \emph{return}, injects an object of type $a$ into an object of type $m a$, where
$m$ is some monad. The second operator takes in an object of type $m a$ on the left hand side, and
a function $f$ from $a$ to $m b$ on the right hand side, and returns an object of type $m b$.
Informally, the operator unwraps the object of type $a$ from the object of type $m a$, and then applies
the function to it to obtain a result.






\section{Summary}

In this chapter we discussed the key ideas of decision support
systems, followed by an overview of artificial reasoning, and an
introduction to Dempster-Shafer theory. We then introduced Subjective
Logic and presented a brief overview of the Haskell programming
language. In the next chapter we present our research motivation,
our thesis hypothesis, our research objectives, and our methodology.


\end{document}
